{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1c4c89ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 close  log_return  rolling_std  realized_volatility\n",
      "Date                                                                \n",
      "2024-11-25  232.614243    0.012966     0.010808             0.171575\n",
      "2024-11-26  234.801834    0.009360     0.011011             0.174787\n",
      "2024-11-27  234.671982   -0.000553     0.010375             0.164701\n",
      "2024-11-29  237.069351    0.010164     0.009480             0.150496\n",
      "2024-12-02  239.326859    0.009478     0.008818             0.139986\n",
      "            vix_close\n",
      "Date                 \n",
      "2024-10-25  20.330000\n",
      "2024-10-28  19.799999\n",
      "2024-10-29  19.340000\n",
      "2024-10-30  20.350000\n",
      "2024-10-31  23.160000\n",
      "    buy_date  current_stock_price  realized_volatility  vix_value  \\\n",
      "0 2024-12-10           247.497879             0.116297      14.18   \n",
      "1 2024-12-11           246.219284             0.120618      13.58   \n",
      "2 2024-12-12           247.687683             0.120655      13.92   \n",
      "3 2024-12-13           247.857483             0.116774      13.81   \n",
      "4 2024-12-16           250.764282             0.097199      14.69   \n",
      "\n",
      "   close_lag_10  close_lag_9  close_lag_8  close_lag_7  close_lag_6  \\\n",
      "0    232.614243   234.801834   234.671982   237.069351   239.326859   \n",
      "1    234.801834   234.671982   237.069351   239.326859   242.383499   \n",
      "2    234.671982   237.069351   239.326859   242.383499   242.743103   \n",
      "3    237.069351   239.326859   242.383499   242.743103   242.773071   \n",
      "4    239.326859   242.383499   242.743103   242.773071   242.573288   \n",
      "\n",
      "   close_lag_5  close_lag_4  close_lag_3  close_lag_2  \n",
      "0   242.383499   242.743103   242.773071   242.573288  \n",
      "1   242.743103   242.773071   242.573288   246.479004  \n",
      "2   242.773071   242.573288   246.479004   247.497879  \n",
      "3   242.573288   246.479004   247.497879   246.219284  \n",
      "4   246.479004   247.497879   246.219284   247.687683  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "\n",
    "def pull_stock_data(ticker, period='6mo', interval='1d'):\n",
    "    stock = yf.Ticker(ticker)\n",
    "    df = stock.history(period=period, interval=interval)\n",
    "    df = df[['Close']].rename(columns={'Close': 'close'})\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    return df\n",
    "\n",
    "def pull_vix_data(period='6mo', interval='1d'):\n",
    "    vix = yf.Ticker('^VIX')\n",
    "    df = vix.history(period=period, interval=interval)\n",
    "    df = df[['Close']].rename(columns={'Close': 'vix_close'})\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    return df\n",
    "\n",
    "def calculate_realized_volatility(df, window=20):\n",
    "    df['log_return'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    df['rolling_std'] = df['log_return'].rolling(window=window, min_periods=window).std()\n",
    "    df['realized_volatility'] = df['rolling_std'] * np.sqrt(252)\n",
    "    df = df.dropna(subset=['realized_volatility']).copy()\n",
    "    return df\n",
    "\n",
    "def generate_stock_price_prediction_dataset(\n",
    "    df,\n",
    "    vix_df,\n",
    "    days_to_predict=5,\n",
    "    lookback_days=10  # how many past closes you use\n",
    "):\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    vix_df = vix_df[['vix_close']]  # Ensure only relevant column is used\n",
    "    df = df.merge(vix_df, left_index=True, right_index=True, how='inner')\n",
    "    df = df.dropna()  # Drop rows where VIX or volatility missing\n",
    "\n",
    "    trading_days = df.index\n",
    "\n",
    "    for i in range(lookback_days, len(df) - days_to_predict):\n",
    "        buy_date = trading_days[i]\n",
    "        S_buy = df.loc[buy_date, 'close']\n",
    "        realized_vol = df.loc[buy_date, 'realized_volatility']\n",
    "        vix_value = df.loc[buy_date, 'vix_close']\n",
    "\n",
    "        # Get the next 5 closes\n",
    "        S_predict_array = df.loc[trading_days[i+1:i+1+days_to_predict], 'close'].values\n",
    "\n",
    "        # Skip if not enough future data\n",
    "        if len(S_predict_array) < days_to_predict:\n",
    "            continue\n",
    "\n",
    "        if np.isnan(S_buy) or np.isnan(realized_vol) or np.isnan(vix_value) or np.any(np.isnan(S_predict_array)):\n",
    "            continue\n",
    "\n",
    "        past_closes = df.loc[trading_days[i - lookback_days:i - 1], 'close'].values\n",
    "\n",
    "        feature_row = {\n",
    "            'buy_date': buy_date,\n",
    "            'current_stock_price': S_buy,\n",
    "            'realized_volatility': realized_vol,\n",
    "            'vix_value': vix_value,\n",
    "        }\n",
    "\n",
    "        # Add past close prices\n",
    "        for j in range(lookback_days-1):\n",
    "            feature_row[f'close_lag_{lookback_days-j}'] = past_closes[j]\n",
    "\n",
    "        features.append(feature_row)\n",
    "        labels.append(S_predict_array)  # target is now an array of 5 prices!\n",
    "\n",
    "    feature_df = pd.DataFrame(features)\n",
    "    label_array = np.array(labels)  # Numpy array: shape (n_samples, 5)\n",
    "\n",
    "    return feature_df, label_array\n",
    "\n",
    "\n",
    "# Full pipeline\n",
    "def create_stock_price_prediction_dataset(ticker, days_to_predict=5):\n",
    "    df = pull_stock_data(ticker, period='6mo', interval='1d')\n",
    "    vix_df = pull_vix_data(period='6mo', interval='1d')\n",
    "    df = calculate_realized_volatility(df)\n",
    "\n",
    "    df.index = df.index.tz_localize(None)\n",
    "    vix_df.index = vix_df.index.tz_localize(None)\n",
    "\n",
    "    print(df.head())\n",
    "    print(vix_df.head())\n",
    "    feature_df, label_series = generate_stock_price_prediction_dataset(\n",
    "        df,\n",
    "        vix_df,\n",
    "        days_to_predict=days_to_predict\n",
    "    )\n",
    "    return feature_df, label_series\n",
    "\n",
    "# Example usage\n",
    "ticker = \"AAPL\"\n",
    "features, labels = create_stock_price_prediction_dataset(ticker)\n",
    "print(features.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "866e6b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[246.21928406, 247.68768311, 247.85748291, 250.76428223,\n",
       "        253.20159912],\n",
       "       [247.68768311, 247.85748291, 250.76428223, 253.20159912,\n",
       "        247.77757263],\n",
       "       [247.85748291, 250.76428223, 253.20159912, 247.77757263,\n",
       "        249.51565552],\n",
       "       [250.76428223, 253.20159912, 247.77757263, 249.51565552,\n",
       "        254.21051025],\n",
       "       [253.20159912, 247.77757263, 249.51565552, 254.21051025,\n",
       "        254.98965454],\n",
       "       [247.77757263, 249.51565552, 254.21051025, 254.98965454,\n",
       "        257.91644287],\n",
       "       [249.51565552, 254.21051025, 254.98965454, 257.91644287,\n",
       "        258.73550415],\n",
       "       [254.21051025, 254.98965454, 257.91644287, 258.73550415,\n",
       "        255.30929565],\n",
       "       [254.98965454, 257.91644287, 258.73550415, 255.30929565,\n",
       "        251.92301941],\n",
       "       [257.91644287, 258.73550415, 255.30929565, 251.92301941,\n",
       "        250.14497375],\n",
       "       [258.73550415, 255.30929565, 251.92301941, 250.14497375,\n",
       "        243.5821991 ],\n",
       "       [255.30929565, 251.92301941, 250.14497375, 243.5821991 ,\n",
       "        243.09272766],\n",
       "       [251.92301941, 250.14497375, 243.5821991 , 243.09272766,\n",
       "        244.73092651],\n",
       "       [250.14497375, 243.5821991 , 243.09272766, 244.73092651,\n",
       "        241.94400024],\n",
       "       [243.5821991 , 243.09272766, 244.73092651, 241.94400024,\n",
       "        242.43344116],\n",
       "       [243.09272766, 244.73092651, 241.94400024, 242.43344116,\n",
       "        236.58987427],\n",
       "       [244.73092651, 241.94400024, 242.43344116, 236.58987427,\n",
       "        234.14256287],\n",
       "       [241.94400024, 242.43344116, 236.58987427, 234.14256287,\n",
       "        233.02378845],\n",
       "       [242.43344116, 236.58987427, 234.14256287, 233.02378845,\n",
       "        237.60874939],\n",
       "       [236.58987427, 234.14256287, 233.02378845, 237.60874939,\n",
       "        228.00930786],\n",
       "       [234.14256287, 233.02378845, 237.60874939, 228.00930786,\n",
       "        229.72741699],\n",
       "       [233.02378845, 237.60874939, 228.00930786, 229.72741699,\n",
       "        222.39547729],\n",
       "       [237.60874939, 228.00930786, 229.72741699, 222.39547729,\n",
       "        223.58416748],\n",
       "       [228.00930786, 229.72741699, 222.39547729, 223.58416748,\n",
       "        223.41436768],\n",
       "       [229.72741699, 222.39547729, 223.58416748, 223.41436768,\n",
       "        222.5353241 ],\n",
       "       [222.39547729, 223.58416748, 223.41436768, 222.5353241 ,\n",
       "        229.60754395],\n",
       "       [223.58416748, 223.41436768, 222.5353241 , 229.60754395,\n",
       "        237.99832153],\n",
       "       [223.41436768, 222.5353241 , 229.60754395, 237.99832153,\n",
       "        239.09712219],\n",
       "       [222.5353241 , 229.60754395, 237.99832153, 239.09712219,\n",
       "        237.32905579],\n",
       "       [229.60754395, 237.99832153, 239.09712219, 237.32905579,\n",
       "        235.74081421],\n",
       "       [237.99832153, 239.09712219, 237.32905579, 235.74081421,\n",
       "        227.75958252],\n",
       "       [239.09712219, 237.32905579, 235.74081421, 227.75958252,\n",
       "        232.54432678],\n",
       "       [237.32905579, 235.74081421, 227.75958252, 232.54432678,\n",
       "        232.21469116],\n",
       "       [235.74081421, 227.75958252, 232.54432678, 232.21469116,\n",
       "        232.96386719],\n",
       "       [227.75958252, 232.54432678, 232.21469116, 232.96386719,\n",
       "        227.38000488],\n",
       "       [232.54432678, 232.21469116, 232.96386719, 227.38000488,\n",
       "        227.6499939 ],\n",
       "       [232.21469116, 232.96386719, 227.38000488, 227.6499939 ,\n",
       "        232.61999512],\n",
       "       [232.96386719, 227.38000488, 227.6499939 , 232.61999512,\n",
       "        236.86999512],\n",
       "       [227.38000488, 227.6499939 , 232.61999512, 236.86999512,\n",
       "        241.52999878],\n",
       "       [227.6499939 , 232.61999512, 236.86999512, 241.52999878,\n",
       "        244.6000061 ],\n",
       "       [232.61999512, 236.86999512, 241.52999878, 244.6000061 ,\n",
       "        244.47000122],\n",
       "       [236.86999512, 241.52999878, 244.6000061 , 244.47000122,\n",
       "        244.86999512],\n",
       "       [241.52999878, 244.6000061 , 244.47000122, 244.86999512,\n",
       "        245.83000183],\n",
       "       [244.6000061 , 244.47000122, 244.86999512, 245.83000183,\n",
       "        245.55000305],\n",
       "       [244.47000122, 244.86999512, 245.83000183, 245.55000305,\n",
       "        247.1000061 ],\n",
       "       [244.86999512, 245.83000183, 245.55000305, 247.1000061 ,\n",
       "        247.03999329],\n",
       "       [245.83000183, 245.55000305, 247.1000061 , 247.03999329,\n",
       "        240.36000061],\n",
       "       [245.55000305, 247.1000061 , 247.03999329, 240.36000061,\n",
       "        237.30000305],\n",
       "       [247.1000061 , 247.03999329, 240.36000061, 237.30000305,\n",
       "        241.83999634],\n",
       "       [247.03999329, 240.36000061, 237.30000305, 241.83999634,\n",
       "        238.02999878],\n",
       "       [240.36000061, 237.30000305, 241.83999634, 238.02999878,\n",
       "        235.92999268],\n",
       "       [237.30000305, 241.83999634, 238.02999878, 235.92999268,\n",
       "        235.74000549],\n",
       "       [241.83999634, 238.02999878, 235.92999268, 235.74000549,\n",
       "        235.33000183],\n",
       "       [238.02999878, 235.92999268, 235.74000549, 235.33000183,\n",
       "        239.07000732],\n",
       "       [235.92999268, 235.74000549, 235.33000183, 239.07000732,\n",
       "        227.47999573],\n",
       "       [235.74000549, 235.33000183, 239.07000732, 227.47999573,\n",
       "        220.83999634],\n",
       "       [235.33000183, 239.07000732, 227.47999573, 220.83999634,\n",
       "        216.97999573],\n",
       "       [239.07000732, 227.47999573, 220.83999634, 216.97999573,\n",
       "        209.67999268],\n",
       "       [227.47999573, 220.83999634, 216.97999573, 209.67999268,\n",
       "        213.49000549],\n",
       "       [220.83999634, 216.97999573, 209.67999268, 213.49000549,\n",
       "        214.        ],\n",
       "       [216.97999573, 209.67999268, 213.49000549, 214.        ,\n",
       "        212.69000244],\n",
       "       [209.67999268, 213.49000549, 214.        , 212.69000244,\n",
       "        215.24000549],\n",
       "       [213.49000549, 214.        , 212.69000244, 215.24000549,\n",
       "        214.1000061 ],\n",
       "       [214.        , 212.69000244, 215.24000549, 214.1000061 ,\n",
       "        218.27000427],\n",
       "       [212.69000244, 215.24000549, 214.1000061 , 218.27000427,\n",
       "        220.72999573],\n",
       "       [215.24000549, 214.1000061 , 218.27000427, 220.72999573,\n",
       "        223.75      ],\n",
       "       [214.1000061 , 218.27000427, 220.72999573, 223.75      ,\n",
       "        221.52999878],\n",
       "       [218.27000427, 220.72999573, 223.75      , 221.52999878,\n",
       "        223.8500061 ],\n",
       "       [220.72999573, 223.75      , 221.52999878, 223.8500061 ,\n",
       "        217.8999939 ],\n",
       "       [223.75      , 221.52999878, 223.8500061 , 217.8999939 ,\n",
       "        222.13000488],\n",
       "       [221.52999878, 223.8500061 , 217.8999939 , 222.13000488,\n",
       "        223.19000244],\n",
       "       [223.8500061 , 217.8999939 , 222.13000488, 223.19000244,\n",
       "        223.88999939],\n",
       "       [217.8999939 , 222.13000488, 223.19000244, 223.88999939,\n",
       "        203.19000244],\n",
       "       [222.13000488, 223.19000244, 223.88999939, 203.19000244,\n",
       "        188.38000488],\n",
       "       [223.19000244, 223.88999939, 203.19000244, 188.38000488,\n",
       "        181.46000671],\n",
       "       [223.88999939, 203.19000244, 188.38000488, 181.46000671,\n",
       "        172.41999817],\n",
       "       [203.19000244, 188.38000488, 181.46000671, 172.41999817,\n",
       "        198.8500061 ],\n",
       "       [188.38000488, 181.46000671, 172.41999817, 198.8500061 ,\n",
       "        190.41999817],\n",
       "       [181.46000671, 172.41999817, 198.8500061 , 190.41999817,\n",
       "        198.1499939 ],\n",
       "       [172.41999817, 198.8500061 , 190.41999817, 198.1499939 ,\n",
       "        202.52000427],\n",
       "       [198.8500061 , 190.41999817, 198.1499939 , 202.52000427,\n",
       "        202.13999939],\n",
       "       [190.41999817, 198.1499939 , 202.52000427, 202.13999939,\n",
       "        194.27000427],\n",
       "       [198.1499939 , 202.52000427, 202.13999939, 194.27000427,\n",
       "        196.97999573],\n",
       "       [202.52000427, 202.13999939, 194.27000427, 196.97999573,\n",
       "        193.16000366],\n",
       "       [202.13999939, 194.27000427, 196.97999573, 193.16000366,\n",
       "        199.74000549],\n",
       "       [194.27000427, 196.97999573, 193.16000366, 199.74000549,\n",
       "        204.6000061 ],\n",
       "       [196.97999573, 193.16000366, 199.74000549, 204.6000061 ,\n",
       "        208.36999512],\n",
       "       [193.16000366, 199.74000549, 204.6000061 , 208.36999512,\n",
       "        209.27999878]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "585fbb8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 12), dtype=tf.float32, name=None), TensorSpec(shape=(None, 5), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def create_lstm_dataset(features_df, labels_array, batch_size=32, shuffle_buffer=1000):\n",
    "    \"\"\"\n",
    "    Create a TensorFlow Dataset for LSTM input.\n",
    "    \n",
    "    Args:\n",
    "        features_df (pd.DataFrame): Feature dataframe (n_samples, n_features).\n",
    "        labels_array (np.ndarray): Label array (n_samples, 5).\n",
    "        batch_size (int): Batch size for training.\n",
    "        shuffle_buffer (int): Buffer size for shuffling.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: Dataset yielding (features, labels) tuples.\n",
    "    \"\"\"\n",
    "    if len(features_df) != len(labels_array):\n",
    "        raise ValueError(\"Features and labels must have the same number of samples.\")\n",
    "    \n",
    "    # Take list of non-numeric columns\n",
    "    non_numeric_columns = features_df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "    # Drop non-numeric columns\n",
    "    features_df = features_df.drop(columns=non_numeric_columns)\n",
    "\n",
    "    features = features_df.to_numpy().astype('float32')\n",
    "    labels = labels_array.astype('float32')\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "    dataset = dataset.shuffle(buffer_size=shuffle_buffer)\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset = create_lstm_dataset(features, labels)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "92a0833e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - loss: 52975.1172\n",
      "Epoch 2/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 52422.9219\n",
      "Epoch 3/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 52333.5156\n",
      "Epoch 4/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 51859.5312 \n",
      "Epoch 5/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 52031.1172\n",
      "Epoch 6/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 51781.3828\n",
      "Epoch 7/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 52094.1758\n",
      "Epoch 8/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 51890.2344\n",
      "Epoch 9/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 51856.1133\n",
      "Epoch 10/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 51346.7188\n",
      "Epoch 11/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 51288.2500\n",
      "Epoch 12/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 50808.9492\n",
      "Epoch 13/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 51231.2539\n",
      "Epoch 14/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 51405.7188\n",
      "Epoch 15/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 50167.5391\n",
      "Epoch 16/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 49857.4844\n",
      "Epoch 17/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 49670.3320\n",
      "Epoch 18/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 49555.1445\n",
      "Epoch 19/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 49719.3281\n",
      "Epoch 20/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 49145.7539\n",
      "Epoch 21/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 48961.7969\n",
      "Epoch 22/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 48182.8672\n",
      "Epoch 23/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 48040.6055\n",
      "Epoch 24/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 47871.3750\n",
      "Epoch 25/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 47314.3906\n",
      "Epoch 26/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 47473.3711\n",
      "Epoch 27/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 46510.6562\n",
      "Epoch 28/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 46553.5781\n",
      "Epoch 29/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 46076.3867\n",
      "Epoch 30/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 45320.4062\n",
      "Epoch 31/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 45487.3203\n",
      "Epoch 32/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 44976.2305\n",
      "Epoch 33/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 43906.0508\n",
      "Epoch 34/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 43718.1602\n",
      "Epoch 35/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 43604.7969\n",
      "Epoch 36/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 43018.9805\n",
      "Epoch 37/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 41984.3320\n",
      "Epoch 38/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 41613.5352\n",
      "Epoch 39/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 41138.8633\n",
      "Epoch 40/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 41167.5078\n",
      "Epoch 41/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 40294.1641\n",
      "Epoch 42/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 38996.4883\n",
      "Epoch 43/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 38975.1445\n",
      "Epoch 44/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 38547.7266\n",
      "Epoch 45/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 37549.1250\n",
      "Epoch 46/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 37450.0078\n",
      "Epoch 47/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 37315.9297\n",
      "Epoch 48/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 36170.6016\n",
      "Epoch 49/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 35460.6992\n",
      "Epoch 50/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 34957.4688\n",
      "Epoch 51/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 34440.7578\n",
      "Epoch 52/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 33814.0938\n",
      "Epoch 53/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 33428.3516\n",
      "Epoch 54/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 33179.5469\n",
      "Epoch 55/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 32002.1816\n",
      "Epoch 56/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 31354.0508\n",
      "Epoch 57/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 30605.7441\n",
      "Epoch 58/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 30472.6445\n",
      "Epoch 59/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 29286.0430\n",
      "Epoch 60/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 28369.2227\n",
      "Epoch 61/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 28373.6465\n",
      "Epoch 62/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 27836.4766\n",
      "Epoch 63/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 27146.4180\n",
      "Epoch 64/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 26465.9082\n",
      "Epoch 65/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 26071.6836\n",
      "Epoch 66/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 25294.0352\n",
      "Epoch 67/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 24523.7441\n",
      "Epoch 68/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 23952.2695\n",
      "Epoch 69/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 23393.5488\n",
      "Epoch 70/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 22670.4727\n",
      "Epoch 71/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 22639.4395\n",
      "Epoch 72/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 21619.4941\n",
      "Epoch 73/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 21300.2129\n",
      "Epoch 74/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 20460.7207\n",
      "Epoch 75/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 19556.9043\n",
      "Epoch 76/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 19175.9570\n",
      "Epoch 77/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 19203.0430\n",
      "Epoch 78/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 18316.7637\n",
      "Epoch 79/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 17501.8164\n",
      "Epoch 80/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 16780.5137\n",
      "Epoch 81/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 16447.0156\n",
      "Epoch 82/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 16047.6250\n",
      "Epoch 83/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 15357.5840\n",
      "Epoch 84/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 14859.2246\n",
      "Epoch 85/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 14640.0996\n",
      "Epoch 86/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 13947.5244\n",
      "Epoch 87/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 13035.5020\n",
      "Epoch 88/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 12867.7002\n",
      "Epoch 89/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 12325.1055\n",
      "Epoch 90/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 12028.5117\n",
      "Epoch 91/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 11617.4150\n",
      "Epoch 92/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 11120.6631\n",
      "Epoch 93/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 10935.3965\n",
      "Epoch 94/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 10297.4160\n",
      "Epoch 95/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 10273.4434\n",
      "Epoch 96/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 9503.4463\n",
      "Epoch 97/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 9216.0508\n",
      "Epoch 98/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 8650.6465\n",
      "Epoch 99/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8311.1621\n",
      "Epoch 100/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7863.9058\n",
      "Epoch 101/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7671.9238\n",
      "Epoch 102/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7414.3569\n",
      "Epoch 103/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 6945.4136\n",
      "Epoch 104/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 6622.4390\n",
      "Epoch 105/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6218.4541\n",
      "Epoch 106/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 6031.3818\n",
      "Epoch 107/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 5814.3403\n",
      "Epoch 108/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 5567.5635\n",
      "Epoch 109/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5100.8594\n",
      "Epoch 110/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 5072.4116\n",
      "Epoch 111/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4916.4248\n",
      "Epoch 112/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4528.3730\n",
      "Epoch 113/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4435.7817\n",
      "Epoch 114/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 4094.1511\n",
      "Epoch 115/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 4040.0830\n",
      "Epoch 116/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 3720.6958\n",
      "Epoch 117/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 3602.3145\n",
      "Epoch 118/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 3214.0601\n",
      "Epoch 119/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 3188.1279\n",
      "Epoch 120/120\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 2925.8838\n"
     ]
    }
   ],
   "source": [
    "# Train LSTM Model:\n",
    "\n",
    "def create_lstm_model(input_shape, output_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(64, return_sequences=True, input_shape=input_shape),\n",
    "        tf.keras.layers.LSTM(32),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(output_shape)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "def train_lstm_model(dataset, input_shape, output_shape, epochs=100):\n",
    "    model = create_lstm_model(input_shape, output_shape)\n",
    "    model.fit(dataset, epochs=epochs)\n",
    "    return model\n",
    "\n",
    "input_shape = (features.shape[1], 1)  # Number of features, 1 timestep\n",
    "output_shape = labels.shape[1]  # Number of days to predict\n",
    "lstm_model = train_lstm_model(dataset, input_shape, output_shape, epochs=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d554a778",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "options",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
